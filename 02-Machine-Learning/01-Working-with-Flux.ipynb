{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../figures/fluxml.png\" width=400 height=140>\n",
    "\n",
    "# Working with Flux\n",
    "In this part of the tutorial, we will show how to work with the Neural Network library called `Flux.jl`. Flux works in a similar way to PyTorch, but is fully built in Julia. It also supports the NVIDIA CUDA GPU backend, and other backends (`AMD`, `Metal`) are actively in development. This Notebook outlines the basic functionality of Flux using the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux, Statistics\n",
    "using MLJ: partition\n",
    "using Plots\n",
    "using MLDatasets: MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading our libraries, we proceed to load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataset MNIST:\n",
       "  metadata  =>    Dict{String, Any} with 3 entries\n",
       "  split     =>    :train\n",
       "  features  =>    28×28×60000 Array{Float32, 3}\n",
       "  targets   =>    60000-element Vector{Int64}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = MNIST(:train, dir=\"../datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split off a test set for real time overfitting monitoring, using the MLJ partition function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx, test_idx = partition(1:length(data.targets), 0.8, rng=123);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the training and testing features and labels. We add a dimension to the features to transform the images from (28x28) to (28x28x1), making them work with 2D convolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ft = data.features[:,:,train_idx];\n",
    "train_ft = reshape(train_ft, (size(train_ft, 1), size(train_ft, 2), 1, size(train_ft, 3)))\n",
    "train_lb = data.targets[train_idx];\n",
    "\n",
    "test_ft = data.features[:,:,test_idx];\n",
    "test_ft = reshape(test_ft, (size(test_ft, 1), size(test_ft, 2), 1, size(test_ft, 3)))\n",
    "test_lb = data.targets[test_idx];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the model using a chain of convolutions, max-pooling operations, a dropout layer and a dense layer to finalize the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Conv((3, 3), 1 => 12, relu),          \u001b[90m# 120 parameters\u001b[39m\n",
       "  MaxPool((2, 2)),\n",
       "  Conv((3, 3), 12 => 24, relu),         \u001b[90m# 2_616 parameters\u001b[39m\n",
       "  MaxPool((2, 2)),\n",
       "  Conv((3, 3), 24 => 48, relu),         \u001b[90m# 10_416 parameters\u001b[39m\n",
       "  MaxPool((2, 2)),\n",
       "  Dropout(0.2),\n",
       "  Flux.flatten,\n",
       "  Dense(48 => 10),                      \u001b[90m# 490 parameters\u001b[39m\n",
       ") \u001b[90m                  # Total: 8 arrays, \u001b[39m13_642 parameters, 54.875 KiB."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define the network\n",
    "model = Chain(\n",
    "  Conv((3,3), 1 => 12, relu),\n",
    "  MaxPool((2,2)),\n",
    "  Conv((3,3), 12 => 24, relu),\n",
    "  MaxPool((2,2)),\n",
    "  Conv((3,3), 24 => 48, relu),\n",
    "  MaxPool((2,2)),\n",
    "  Dropout(0.2),\n",
    "  Flux.flatten,\n",
    "  Dense(48 => 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our model contains 13,642 parameters that can be trained. Now we convert our labels to one-hot format, and define a dataloader for training on the images in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element DataLoader(::Tuple{Array{Float32, 4}, OneHotArrays.OneHotMatrix{UInt32, Vector{UInt32}}}, shuffle=true, batchsize=20000)\n",
       "  with first element:\n",
       "  (28×28×1×20000 Array{Float32, 4}, 10×20000 OneHotMatrix(::Vector{UInt32}) with eltype Bool,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_targets = Flux.onehotbatch(train_lb, 0:9)\n",
    "test_targets = Flux.onehotbatch(test_lb, 0:9)\n",
    "train_sources = Flux.DataLoader((train_ft, train_targets), batchsize = 20_000, shuffle = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the optimizer. We selected Adam with a learning rate of 0.01 for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = Flux.setup(Flux.Adam(0.01), model);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the train loop, while saving the losses and test losses to check for overfitting behavior. \n",
    "\n",
    "**Warning**: This block can easily take long to run. We have set epochs to 5 to make it run more quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n",
      "\tLoss: 2.27897\n",
      "\tTest Loss: 2.1610935\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4\n",
      "\tLoss: 1.4822559\n",
      "\tTest Loss: 0.95761377\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6\n",
      "\tLoss: 0.81905776\n",
      "\tTest Loss: 0.5772222\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8\n",
      "\tLoss: 0.5859837\n",
      "\tTest Loss: 0.39168856\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10\n",
      "\tLoss: 0.43784252\n",
      "\tTest Loss: 0.28471595\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12\n",
      "\tLoss: 0.3542919\n",
      "\tTest Loss: 0.2280229\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14\n",
      "\tLoss: 0.2931049\n",
      "\tTest Loss: 0.19243355\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16\n",
      "\tLoss: 0.24965803\n",
      "\tTest Loss: 0.17043787\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18\n",
      "\tLoss: 0.2230231\n",
      "\tTest Loss: 0.14846373\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20\n",
      "\tLoss: 0.20022975\n",
      "\tTest Loss: 0.13595696\n",
      "\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "val_losses = []\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "for epoch in 1:epochs\n",
    "  if epoch % 2 == 0\n",
    "    println(\"Epoch $(epoch)\\n\\tLoss: $(losses[end])\\n\\tTest Loss: $(val_losses[end])\\n\")\n",
    "  end\n",
    "  epoch_loss = []\n",
    "  for (x, y) in train_sources\n",
    "    loss, grads = Flux.withgradient(model) do m\n",
    "      ŷ = m(x)\n",
    "      Flux.logitcrossentropy(ŷ, y)\n",
    "    end\n",
    "    Flux.update!(optim, model, grads[1])\n",
    "    push!(epoch_loss, loss)\n",
    "  end\n",
    "  \n",
    "  ŷ_val = model(test_ft)\n",
    "  val_loss = Flux.logitcrossentropy(ŷ_val, test_targets)\n",
    "  push!(losses, mean(epoch_loss))\n",
    "  push!(val_losses, val_loss)\n",
    "end\n",
    "\n",
    "#gif(anim, \"../figures/losscurve.gif\", fps=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can visualize our training curves, showing no overfitting behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anim = @animate for epoch in 1:epochs\n",
    "#   pl = plot(losses[1:epoch], xlabel=\"Epoch\", ylabel=\"Loss\", title=\"Training MNIST with Flux.jl\",\n",
    "#   titlefontfamily=\"Helvetica Bold\", titlefontsize=12, guidefontfamily=\"Helvetica Bold\", guidefontsize=10,\n",
    "#   label=\"Train\")\n",
    "#   plot!(val_losses[1:epoch], label=\"Test\")\n",
    "# end\n",
    "\n",
    "# gif(anim, \"../figures/losscurve.gif\", fps=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the code for 100 epochs (do not try this on the CPU, it will take way too long) will produce the following figure\n",
    "\n",
    "![Loss Curve](../figures/losscurve.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Exercise\n",
    "---\n",
    "\n",
    "*This exercise is meant for those without GPU access. In case you have a CUDA-enabled GPU you can access, you can move on with the GPU acceleration notebook and do the exercise there.*\n",
    "\n",
    "Load in the CrabAge dataset and try to train a simple neural network to predict the age of the crabs. Also try to use the animation functionality of Plots to visualize your loss curve evolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working on it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.0",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
